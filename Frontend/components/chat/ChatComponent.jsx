import { useState, useEffect, useRef } from "react";
import OpenAI from 'openai';
import MessageList from "./MessageList";
import MessageInput from "./MessageInput";
import { ChatOpenAI } from "@langchain/openai";
import { HumanMessage, AIMessage, SystemMessage } from "@langchain/core/messages";
import { BufferMemory } from "langchain/memory";
import { COMBINED_THERAPEUTIC_PROMPT } from "./prompts";
import config, { validateConfig } from "../../src/config";

const ChatComponent = () => {
  const [messages, setMessages] = useState([
    { from: "bot", text: "Hola, soy EMILIA. ¬øC√≥mo te sientes hoy? Estoy aqu√≠ para escucharte y ayudarte." },
  ]);
  const [isLoading, setIsLoading] = useState(false);
  const [configError, setConfigError] = useState(false);
  const messagesEndRef = useRef(null);
  
  // Referencia a la instancia de OpenAI para OpenRouter
  const openRouter = useRef(
    config.AI.PROVIDER === 'openrouter' 
      ? new OpenAI({
          baseURL: config.AI.OPENROUTER.BASE_URL,
          apiKey: config.AI.OPENROUTER.API_KEY,
          defaultHeaders: {
            'HTTP-Referer': config.AI.OPENROUTER.SITE_URL,
            'X-Title': config.AI.OPENROUTER.SITE_NAME,
          },
          dangerouslyAllowBrowser: true,
        })
      : null
  );
  
  // Validar la configuraci√≥n al iniciar
  useEffect(() => {
    const isConfigValid = validateConfig();
    setConfigError(!isConfigValid);
    
    if (!isConfigValid) {
      console.error('‚ùå Error en la configuraci√≥n. Verifica el archivo .env');
    } else if (config.APP.DEBUG_MODE) {
      console.log('‚úÖ Configuraci√≥n v√°lida.');
      console.log(`ü§ñ Usando proveedor: ${config.AI.PROVIDER}`);
      console.log(`üß† Usando modelo: ${config.AI.PROVIDER === 'openai' ? config.AI.MODEL : config.AI.OPENROUTER.MODEL}`);
    }
  }, []);
  
  // Initialize LangChain chat model (solo para OpenAI)
  const chatModel = useRef(
    config.AI.PROVIDER === 'openai'
      ? new ChatOpenAI({
          modelName: config.AI.MODEL,
          temperature: config.AI.TEMPERATURE,
          openAIApiKey: config.AI.API_KEY,
        })
      : null
  );
  
  // Initialize memory to store chat history
  const memory = useRef(new BufferMemory({
    returnMessages: true,
    memoryKey: "chat_history",
  }));

  useEffect(() => {
    messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
  }, [messages]);
  
  // Funci√≥n para procesar y a√±adir respuestas m√∫ltiples con retraso
  const addBotMessagesWithDelay = async (responseText, userMessage) => {
    // Dividir la respuesta por el separador "---"
    const messageSegments = responseText.split(/\s*---\s*/);
    
    // A√±adir cada segmento como un mensaje separado con retraso
    for (let i = 0; i < messageSegments.length; i++) {
      const segment = messageSegments[i].trim();
      if (segment) {
        // Si no es el primer mensaje, a√±adir un retraso adaptativo
        if (i > 0) {
          // Calcular retraso basado en la longitud del mensaje anterior
          // Mensajes cortos = retraso corto, mensajes largos = retraso m√°s largo
          const prevSegmentLength = messageSegments[i-1].length;
          // Retraso m√≠nimo de 800ms, m√°ximo de 2000ms
          const delay = Math.min(Math.max(800, prevSegmentLength * 3), 2000);
          
          // Para desarrollo, podemos usar un retraso m√°s corto para pruebas
          const finalDelay = config.APP.DEBUG_MODE ? Math.min(delay, 1000) : delay;
          
          await new Promise(resolve => setTimeout(resolve, finalDelay));
        }
        
        setMessages(prev => [...prev, { from: "bot", text: segment }]);
        
        // Actualizar el contexto de memoria solo con el √∫ltimo mensaje para evitar duplicaci√≥n
        if (i === messageSegments.length - 1) {
          await memory.current.saveContext(
            { input: userMessage },
            { output: responseText } // Guardar respuesta completa en la memoria
          );
        }
        
        // Scroll al fondo despu√©s de a√±adir cada mensaje
        setTimeout(() => {
          messagesEndRef.current?.scrollIntoView({ behavior: "smooth" });
        }, 100);
      }
    }
  };

  const handleSendMessage = async (message) => {
    if (!message.trim()) return;
    
    // Add user message to the state
    setMessages((prev) => [...prev, { from: "user", text: message }]);
    setIsLoading(true);

    try {
      if (configError) {
        throw new Error("Error de configuraci√≥n. Verifica el archivo .env");
      }
      
      let botResponseText = "";
      
      if (config.AI.PROVIDER === 'openai') {
        // Usar LangChain con OpenAI
        const langchainMessages = [
          new SystemMessage(COMBINED_THERAPEUTIC_PROMPT),
          ...formatMessagesForLangChain()
        ];
        
        langchainMessages.push(new HumanMessage(message));
        
        const response = await chatModel.current.call(langchainMessages);
        botResponseText = response.content;
        
      } else if (config.AI.PROVIDER === 'openrouter') {
        // Usar OpenRouter directamente
        const conversationHistory = formatMessagesForOpenRouter();
        
        // A√±adir el prompt del sistema al principio
        if (conversationHistory.length === 0 || conversationHistory[0].role !== 'system') {
          conversationHistory.unshift({
            role: 'system',
            content: COMBINED_THERAPEUTIC_PROMPT
          });
        }
        
        // A√±adir el mensaje del usuario
        conversationHistory.push({
          role: 'user',
          content: message
        });
        
        // Llamar a OpenRouter con el modelo Gemini
        const completion = await openRouter.current.chat.completions.create({
          model: config.AI.OPENROUTER.MODEL,
          messages: conversationHistory,
          temperature: config.AI.TEMPERATURE,
        });
        
        botResponseText = completion.choices[0].message.content;
      } else {
        throw new Error(`Proveedor desconocido: ${config.AI.PROVIDER}`);
      }
      
      // Verificar si la respuesta contiene el separador "---"
      if (botResponseText.includes('---')) {
        // Procesar m√∫ltiples mensajes
        await addBotMessagesWithDelay(botResponseText, message);
      } else {
        // A√±adir un solo mensaje como antes
        setMessages((prev) => [...prev, { from: "bot", text: botResponseText }]);
        
        // Actualizar la memoria
        await memory.current.saveContext(
          { input: message },
          { output: botResponseText }
        );
      }
      
    } catch (error) {
      console.error("Error getting response from AI:", error);
      let errorMessage = "Lo siento, hubo un error al procesar tu mensaje. Por favor, intenta de nuevo.";
      
      if (configError || error.message.includes('API key')) {
        errorMessage = "Error de configuraci√≥n: Verifica que la API key y el modelo est√©n correctamente configurados en el archivo .env";
      }
      
      setMessages((prev) => [
        ...prev,
        { from: "bot", text: errorMessage },
      ]);
    } finally {
      setIsLoading(false);
    }
  };
  
  // Helper function to format messages for LangChain
  const formatMessagesForLangChain = () => {
    // Skip the first message (bot greeting) and start from user messages
    return messages.slice(1).map(msg => {
      if (msg.from === "user") {
        return new HumanMessage(msg.text);
      } else {
        return new AIMessage(msg.text);
      }
    });
  };
  
  // Helper function to format messages for OpenRouter
  const formatMessagesForOpenRouter = () => {
    // Para la historia de la conversaci√≥n, tratamos todos los mensajes como entidades independientes
    // No incluimos el mensaje del sistema aqu√≠, lo a√±adiremos por separado
    
    // Crearemos un nuevo array con los mensajes formateados
    const formattedMessages = [];
    
    // Recorremos todos los mensajes en la conversaci√≥n
    messages.forEach(msg => {
      formattedMessages.push({
        role: msg.from === "user" ? "user" : "assistant",
        content: msg.text
      });
    });
    
    return formattedMessages;
  };

  return (
    <div style={styles.chatContainer}>
      {/* ENCABEZADO DEL CHAT */}
      <div style={styles.header}>
        <h2 style={styles.headerText}>EMILIA - Asistente Terap√©utico</h2>
        <p style={styles.headerSubtext}>
          {configError 
            ? "‚ö†Ô∏è Error de configuraci√≥n. Verifica el archivo .env" 
            : "Un espacio seguro para expresarte y recibir apoyo"}
        </p>
      </div>

      {/* √ÅREA DE MENSAJES */}
      <div style={styles.messageList}>
        <MessageList messages={messages} />
        <div ref={messagesEndRef} />
        {isLoading && <div style={styles.loadingIndicator}>EMILIA est√° reflexionando...</div>}
      </div>

      {/* CAJA DE MENSAJE */}
      <MessageInput 
        onSendMessage={handleSendMessage} 
        isDisabled={isLoading || configError} 
        placeholder={configError 
          ? "Configuraci√≥n incompleta. Contacta al administrador."
          : "Comparte c√≥mo te sientes hoy..."
        }
      />
    </div>
  );
};

const styles = {
  chatContainer: {
    display: "flex",
    flexDirection: "column",
    height: "90vh", 
    width: "calc(100% - 20px)",
    borderRadius: "12px",
    background: "#f8f9fa",
    overflow: "hidden", 
    margin: "auto", 
    boxShadow: "0 2px 10px rgba(0,0,0,0.1)",
  },
  header: {
    background: "linear-gradient(135deg, #7e57c2 0%, #9b59b6 100%)",
    color: "white",
    padding: "15px 20px",
    textAlign: "center",
  },
  headerText: {
    margin: 0,
    color: "white",
    fontSize: "1.5rem",
    fontWeight: "600",
  },
  headerSubtext: {
    margin: "5px 0 0 0",
    color: "rgba(255, 255, 255, 0.9)",
    fontSize: "0.9rem",
    fontWeight: "normal",
  },
  modelInfo: {
    color: "rgba(255, 255, 255, 0.7)",
    fontSize: "0.8rem",
    display: "block",
    marginTop: "4px",
  },
  messageList: {
    flex: 1,
    overflowY: "auto",
    overflowX: "hidden",
    padding: "15px",
    maxHeight: "calc(100vh - 180px)",
    backgroundColor: "#f8f9fa",
  },
  loadingIndicator: {
    padding: "10px",
    textAlign: "center",
    fontStyle: "italic",
    color: "#666",
    fontSize: "0.9rem",
  },
  "@media (max-width: 768px)": {
    chatContainer: {
      width: "100%", 
      height: "100vh", 
      borderRadius: "0",
    },
  },
  "@media (max-width: 1024px)": {
    chatContainer: {
      width: "calc(100% - 10px)",
    },
  },
};

export default ChatComponent;
